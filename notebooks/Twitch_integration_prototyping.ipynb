{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Twitch_integration_prototyping.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwFXS0vUzsfO"
      },
      "source": [
        "# MakeItTalk + text to speech and speech recognition\n",
        "\n",
        "- included project setup + pretrained model download\n",
        "- provides step-by-step details\n",
        "- todo: tdlr version"
      ],
      "id": "dwFXS0vUzsfO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVAYGCMEwDwB"
      },
      "source": [
        "Start make it talk\n"
      ],
      "id": "NVAYGCMEwDwB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVedbX194R-A"
      },
      "source": [
        "print('Git clone project and install requirements...')\n",
        "!git clone https://github.com/yzhou359/MakeItTalk &> /dev/null\n",
        "%cd MakeItTalk/\n",
        "!export PYTHONPATH=/content/MakeItTalk:$PYTHONPATH\n",
        "!pip install -r requirements.txt &> /dev/null\n",
        "!pip install tensorboardX &> /dev/null\n",
        "!mkdir examples/dump\n",
        "!mkdir examples/ckpt\n",
        "!pip install gdown &> /dev/null\n",
        "print('Done!')\n",
        "print('Download pre-trained models...') \n",
        "!gdown -O examples/ckpt/ckpt_autovc.pth https://drive.google.com/uc?id=1ZiwPp_h62LtjU0DwpelLUoodKPR85K7x\n",
        "!gdown -O examples/ckpt/ckpt_content_branch.pth https://drive.google.com/uc?id=1r3bfEvTVl6pCNw5xwUhEglwDHjWtAqQp\n",
        "!gdown -O examples/ckpt/ckpt_speaker_branch.pth https://drive.google.com/uc?id=1rV0jkyDqPW-aDJcj7xSO6Zt1zSXqn1mu\n",
        "!gdown -O examples/ckpt/ckpt_116_i2i_comb.pth https://drive.google.com/uc?id=1i2LJXKp-yWKIEEgJ7C6cE3_2NirfY_0a\n",
        "!gdown -O examples/dump/emb.pickle https://drive.google.com/uc?id=18-0CYl5E6ungS3H4rRSHjfYvvm-WwjTI\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "import subprocess\n",
        "print(subprocess.getoutput('nvidia-smi'))\n",
        "print('Done!')"
      ],
      "id": "QVedbX194R-A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW4ktjHB89Du"
      },
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/datasets.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install torchaudio\n",
        "!pip install librosa\n",
        "!pip install jiwer\n",
        "!pip install ffmpeg-python\n"
      ],
      "id": "aW4ktjHB89Du",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvnAhX7RTky-"
      },
      "source": [
        "Here we are performing speech recognition to get the utterance.\n",
        "\n"
      ],
      "id": "YvnAhX7RTky-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaSJEWDf4R03"
      },
      "source": [
        "\n",
        "import torch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\").to('cuda')\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "def map_to_array(batch):\n",
        "    speech, _ = sf.read(batch[\"file\"])\n",
        "    batch[\"speech\"] = speech\n",
        "    return batch\n",
        "\n",
        "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "ds = ds.map(map_to_array)\n",
        "\n",
        "input_values = processor(ds[\"speech\"][0], return_tensors=\"pt\").input_values  # Batch size 1\n",
        "logits = model(input_values).logits\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "transcription = processor.decode(predicted_ids[0])\n",
        "\n",
        "# compute loss\n",
        "target_transcription = \"A MAN SAID TO THE UNIVERSE SIR I EXIST\"\n",
        "\n",
        "# wrap processor as target processor to encode labels\n",
        "with processor.as_target_processor():\n",
        "    labels = processor(transcription, return_tensors=\"pt\").input_ids\n",
        "\n",
        "loss = model(input_values, labels=labels).loss\n",
        "transcription\n",
        "loss #TODO this needs to be looked in to or not used for this tech demo because it really isn't required."
      ],
      "id": "vaSJEWDf4R03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6epEN6BbGn2"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from jiwer import wer\n",
        "\n",
        "\n",
        "librispeech_eval = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(\"cuda\")\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "def map_to_array(batch):\n",
        "    speech, _ = sf.read(batch[\"file\"])\n",
        "    batch[\"speech\"] = speech\n",
        "    return batch\n",
        "\n",
        "librispeech_eval = librispeech_eval.map(map_to_array)\n",
        "\n",
        "def map_to_pred(batch):\n",
        "    input_values = tokenizer(batch[\"speech\"], return_tensors=\"pt\", padding=\"longest\").input_values\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values.to(\"cuda\")).logits\n",
        "\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = tokenizer.batch_decode(predicted_ids)\n",
        "    batch[\"transcription\"] = transcription\n",
        "    return batch\n",
        "\n",
        "result = librispeech_eval.map(map_to_pred, batched=True, batch_size=1, remove_columns=[\"speech\"])\n",
        "\n",
        "print(\"WER:\", wer(result[\"text\"], result[\"transcription\"]))"
      ],
      "id": "p6epEN6BbGn2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbNQC_L2TwNW"
      },
      "source": [
        "Based on the recognised transcription we pass this into blenderbot-400-distill\n"
      ],
      "id": "lbNQC_L2TwNW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNi7NnsTYWIC"
      },
      "source": [
        ""
      ],
      "id": "eNi7NnsTYWIC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb-UjOzD-zjT"
      },
      "source": [
        "\n",
        "from transformers import (\n",
        "    TFAutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    pipeline,\n",
        "    BlenderbotTokenizer,\n",
        "    BlenderbotSmallTokenizer,\n",
        "    BlenderbotForConditionalGeneration,\n",
        "    Conversation,\n",
        ")\n",
        "#from transformers import RobertaTokenizer, IBertModel\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import pipeline, Conversation\n",
        "#import logging\n",
        "from typing import List\n",
        "\n",
        "#conversational_pipeline = pipeline(\"conversational\", device=0)\n",
        "\n",
        "def blenderbot400M(utterance: str) -> List[str]:\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\").to('cuda')\n",
        "    inputs = tokenizer([utterance], return_tensors=\"pt\")\n",
        "    reply_ids = model.generate(**inputs)\n",
        "    responses = [\n",
        "        tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "        for g in reply_ids\n",
        "    ]\n",
        "    return responses\n",
        "\n",
        "blenderbot400M(transcription[1])"
      ],
      "id": "Zb-UjOzD-zjT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ0yNirUU6dV"
      },
      "source": [
        "blenderbot400M(transcription)"
      ],
      "id": "lJ0yNirUU6dV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93HLLOhSV5B8"
      },
      "source": [
        "Here we download mozillas TTS and use one of their models"
      ],
      "id": "93HLLOhSV5B8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLFK2Re2cXeu"
      },
      "source": [
        "!gdown --id 1dntzjWFg7ufWaTaFy80nRz-Tu02xWZos -O tts_model.pth.tar\n",
        "!gdown --id 18CQ6G6tBEOfvCHlPqP8EBI4xWbrr9dBc -O config.json\n",
        "!gdown --id 1Ty5DZdOc0F7OTGj9oJThYbL5iVu_2G0K -O vocoder_model.pth.tar\n",
        "!gdown --id 1Rd0R_nRCrbjEdpOwq6XwZAktvugiBvmu -O config_vocoder.json\n",
        "!gdown --id 11oY3Tv0kQtxK_JPgxrfesa99maVXHNxU -O scale_stats.npy\n",
        "\n",
        "! sudo apt-get install espeak\n",
        "!git clone https://github.com/coqui-ai/TTS"
      ],
      "id": "xLFK2Re2cXeu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr5AvZq3EI8o"
      },
      "source": [
        "\n",
        "%cd TTS\n",
        "!git checkout b1935c97\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py install\n",
        "%cd ..\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "import IPython\n",
        "\n",
        "from TTS.utils.generic_utils import setup_model\n",
        "from TTS.utils.io import load_config\n",
        "from TTS.utils.text.symbols import symbols, phonemes\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "from TTS.utils.synthesis import synthesis\n",
        "from TTS.vocoder.utils.generic_utils import setup_generator\n",
        "\n",
        "# runtime settings\n",
        "use_cuda = False\n",
        "# model paths\n",
        "TTS_MODEL = \"tts_model.pth.tar\"\n",
        "TTS_CONFIG = \"config.json\"\n",
        "VOCODER_MODEL = \"vocoder_model.pth.tar\"\n",
        "VOCODER_CONFIG = \"config_vocoder.json\"\n",
        "# load configs\n",
        "TTS_CONFIG = load_config(TTS_CONFIG)\n",
        "VOCODER_CONFIG = load_config(VOCODER_CONFIG)\n",
        "# load the audio processor\n",
        "ap = AudioProcessor(**TTS_CONFIG.audio)\n",
        "# LOAD TTS MODEL\n",
        "# multi speaker \n",
        "speaker_id = None\n",
        "speakers = []\n",
        "\n",
        "# load the model\n",
        "num_chars = len(phonemes) if TTS_CONFIG.use_phonemes else len(symbols)\n",
        "model = setup_model(num_chars, len(speakers), TTS_CONFIG)\n",
        "\n",
        "# load model state\n",
        "cp =  torch.load(TTS_MODEL, map_location=torch.device('cpu'))\n",
        "\n",
        "# load the model\n",
        "model.load_state_dict(cp['model'])\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "model.eval()\n",
        "\n",
        "# set model stepsize\n",
        "if 'r' in cp:\n",
        "    model.decoder.set_r(cp['r'])\n",
        "\n",
        "# LOAD VOCODER MODEL\n",
        "vocoder_model = setup_generator(VOCODER_CONFIG)\n",
        "vocoder_model.load_state_dict(torch.load(VOCODER_MODEL, map_location=\"cpu\")[\"model\"])\n",
        "vocoder_model.remove_weight_norm()\n",
        "vocoder_model.inference_padding = 0\n",
        "\n",
        "ap_vocoder = AudioProcessor(**VOCODER_CONFIG['audio'])    \n",
        "if use_cuda:\n",
        "    vocoder_model.cuda()\n",
        "vocoder_model.eval()\n",
        "\n",
        "def tts(model, text, CONFIG, use_cuda, ap, use_gl, figures=True):\n",
        "    t_1 = time.time()\n",
        "    waveform, alignment, mel_spec, mel_postnet_spec, stop_tokens, inputs = synthesis(model, text, CONFIG, use_cuda, ap, speaker_id, style_wav=None,\n",
        "                                                                             truncated=False, enable_eos_bos_chars=CONFIG.enable_eos_bos_chars)\n",
        "    # mel_postnet_spec = ap._denormalize(mel_postnet_spec.T)\n",
        "    if not use_gl:\n",
        "        waveform = vocoder_model.inference(torch.FloatTensor(mel_postnet_spec.T).unsqueeze(0))\n",
        "        waveform = waveform.flatten()\n",
        "    if use_cuda:\n",
        "        waveform = waveform.cpu()\n",
        "    waveform = waveform.numpy()\n",
        "    rtf = (time.time() - t_1) / (len(waveform) / ap.sample_rate)\n",
        "    tps = (time.time() - t_1) / len(waveform)\n",
        "    print(waveform.shape)\n",
        "    print(\" > Run-time: {}\".format(time.time() - t_1))\n",
        "    print(\" > Real-time factor: {}\".format(rtf))\n",
        "    print(\" > Time per step: {}\".format(tps))\n",
        "    IPython.display.display(IPython.display.Audio(waveform, rate=CONFIG.audio['sample_rate']))  \n",
        "    return alignment, mel_postnet_spec, stop_tokens, waveform\n",
        "\n",
        "sentence =  \"Bill got in the habit of asking himself “Is that thought true?” and if he wasn’t absolutely certain it was, he just let it go.\"\n",
        "align, spec, stop_tokens, wav = tts(model, sentence, TTS_CONFIG, use_cuda, ap, use_gl=False, figures=True)\n",
        "#align, spec, stop_tokens, wav = tts(model, blenderbot400M(transcription)[0], TTS_CONFIG, use_cuda, ap, use_gl=False, figures=True)\n",
        "\n",
        "\n"
      ],
      "id": "Mr5AvZq3EI8o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs_dYZWzc2d9"
      },
      "source": [
        "!pip install scipy\n",
        "from scipy.io.wavfile import write\n",
        "samplerate = 22050; #fs = 100\n",
        "write('/content/test.wav', samplerate, wav)"
      ],
      "id": "zs_dYZWzc2d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lNScvXmex0g"
      },
      "source": [
        "print('Git clone project and install requirements...')\n",
        "!git clone https://github.com/yzhou359/MakeItTalk &> /dev/null\n",
        "%cd MakeItTalk/\n",
        "!export PYTHONPATH=/content/MakeItTalk:$PYTHONPATH\n",
        "!pip install -r requirements.txt &> /dev/null\n",
        "!pip install tensorboardX &> /dev/null\n",
        "!mkdir examples/dump\n",
        "!mkdir examples/ckpt\n",
        "!pip install gdown &> /dev/null\n",
        "print('Done!')\n",
        "print('Download pre-trained models...')\n",
        "!gdown -O examples/ckpt/ckpt_autovc.pth https://drive.google.com/uc?id=1ZiwPp_h62LtjU0DwpelLUoodKPR85K7x\n",
        "!gdown -O examples/ckpt/ckpt_content_branch.pth https://drive.google.com/uc?id=1r3bfEvTVl6pCNw5xwUhEglwDHjWtAqQp\n",
        "!gdown -O examples/ckpt/ckpt_speaker_branch.pth https://drive.google.com/uc?id=1rV0jkyDqPW-aDJcj7xSO6Zt1zSXqn1mu\n",
        "!gdown -O examples/ckpt/ckpt_116_i2i_comb.pth https://drive.google.com/uc?id=1i2LJXKp-yWKIEEgJ7C6cE3_2NirfY_0a\n",
        "!gdown -O examples/dump/emb.pickle https://drive.google.com/uc?id=18-0CYl5E6ungS3H4rRSHjfYvvm-WwjTI\n",
        "print('Done!')"
      ],
      "id": "4lNScvXmex0g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRZ-NZypOuIN"
      },
      "source": [
        "import ipywidgets as widgets\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Choose the image name to animate: (saved in folder 'examples/')\")\n",
        "img_list = glob.glob1('examples', '*.jpg')\n",
        "img_list.sort()\n",
        "img_list = [item.split('.')[0] for item in img_list]\n",
        "default_head_name = widgets.Dropdown(options=img_list, value='paint_boy')\n",
        "def on_change(change):\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "        plt.imshow(plt.imread('examples/{}.jpg'.format(default_head_name.value)))\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "default_head_name.observe(on_change)\n",
        "display(default_head_name)\n",
        "plt.imshow(plt.imread('examples/{}.jpg'.format(default_head_name.value)))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "id": "iRZ-NZypOuIN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkH0WKbDhO5t"
      },
      "source": [
        "\n",
        "#@markdown # Animation Controllers\n",
        "#@markdown Amplify the lip motion in horizontal direction\n",
        "AMP_LIP_SHAPE_X = 2.8 #@param {type:\"slider\", min:0.5, max:5.0, step:0.1}\n",
        "\n",
        "#@markdown Amplify the lip motion in vertical direction\n",
        "AMP_LIP_SHAPE_Y = 2 #@param {type:\"slider\", min:0.5, max:5.0, step:0.1}\n",
        "\n",
        "#@markdown Amplify the head pose motion (usually smaller than 1.0, put it to 0. for a static head pose)\n",
        "AMP_HEAD_POSE_MOTION = 0.35 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "\n",
        "#@markdown Add naive eye blink\n",
        "ADD_NAIVE_EYE = True  #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n",
        "#@markdown If your image has an opened mouth, put this as True, else False\n",
        "CLOSE_INPUT_FACE_MOUTH = False  #@param [\"False\", \"True\"] {type:\"raw\"}          \n",
        "\n",
        "\n",
        "#@markdown # Landmark Adjustment\n",
        "\n",
        "#@markdown Adjust upper lip thickness (postive value means thicker)\n",
        "UPPER_LIP_ADJUST = -1 #@param {type:\"slider\", min:-3.0, max:3.0, step:1.0}\n",
        "\n",
        "#@markdown Adjust lower lip thickness (postive value means thicker)\n",
        "LOWER_LIP_ADJUST = -1 #@param {type:\"slider\", min:-3.0, max:3.0, step:1.0}\n",
        "\n",
        "#@markdown Adjust static lip width (in multipication)\n",
        "LIP_WIDTH_ADJUST = 1.0 #@param {type:\"slider\", min:0.8, max:1.2, step:0.01}"
      ],
      "id": "mkH0WKbDhO5t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFajpH1aOyjs"
      },
      "source": [
        ""
      ],
      "id": "BFajpH1aOyjs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAfQWE6jO1IF"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"thirdparty/AdaptiveWingLoss\")\n",
        "import os, glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import argparse\n",
        "from src.approaches.train_image_translation import Image_translation_block\n",
        "import torch\n",
        "import pickle\n",
        "import face_alignment\n",
        "from src.autovc.AutoVC_mel_Convertor_retrain_version import AutoVC_mel_Convertor\n",
        "import shutil\n",
        "import time\n",
        "import util.utils as util\n",
        "from scipy.signal import savgol_filter\n",
        "from src.approaches.train_audio2landmark import Audio2landmark_model\n",
        "\n",
        "sys.stdout = open(os.devnull, 'a')\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--jpg', type=str, default='{}.jpg'.format(default_head_name.value))\n",
        "parser.add_argument('--close_input_face_mouth', default=CLOSE_INPUT_FACE_MOUTH, action='store_true')\n",
        "parser.add_argument('--load_AUTOVC_name', type=str, default='examples/ckpt/ckpt_autovc.pth')\n",
        "parser.add_argument('--load_a2l_G_name', type=str, default='examples/ckpt/ckpt_speaker_branch.pth')\n",
        "parser.add_argument('--load_a2l_C_name', type=str, default='examples/ckpt/ckpt_content_branch.pth') #ckpt_audio2landmark_c.pth')\n",
        "parser.add_argument('--load_G_name', type=str, default='examples/ckpt/ckpt_116_i2i_comb.pth') #ckpt_image2image.pth') #ckpt_i2i_finetune_150.pth') #c\n",
        "parser.add_argument('--amp_lip_x', type=float, default=AMP_LIP_SHAPE_X)\n",
        "parser.add_argument('--amp_lip_y', type=float, default=AMP_LIP_SHAPE_Y)\n",
        "parser.add_argument('--amp_pos', type=float, default=AMP_HEAD_POSE_MOTION)\n",
        "parser.add_argument('--reuse_train_emb_list', type=str, nargs='+', default=[]) #  ['iWeklsXc0H8']) #['45hn7-LXDX8']) #['E_kmpT-EfOg']) #'iWeklsXc0H8', '29k8RtSUjE0', '45hn7-LXDX8',\n",
        "parser.add_argument('--add_audio_in', default=False, action='store_true')\n",
        "parser.add_argument('--comb_fan_awing', default=False, action='store_true')\n",
        "parser.add_argument('--output_folder', type=str, default='examples')\n",
        "parser.add_argument('--test_end2end', default=True, action='store_true')\n",
        "parser.add_argument('--dump_dir', type=str, default='', help='')\n",
        "parser.add_argument('--pos_dim', default=7, type=int)\n",
        "parser.add_argument('--use_prior_net', default=True, action='store_true')\n",
        "parser.add_argument('--transformer_d_model', default=32, type=int)\n",
        "parser.add_argument('--transformer_N', default=2, type=int)\n",
        "parser.add_argument('--transformer_heads', default=2, type=int)\n",
        "parser.add_argument('--spk_emb_enc_size', default=16, type=int)\n",
        "parser.add_argument('--init_content_encoder', type=str, default='')\n",
        "parser.add_argument('--lr', type=float, default=1e-3, help='learning rate')\n",
        "parser.add_argument('--reg_lr', type=float, default=1e-6, help='weight decay')\n",
        "parser.add_argument('--write', default=False, action='store_true')\n",
        "parser.add_argument('--segment_batch_size', type=int, default=1, help='batch size')\n",
        "parser.add_argument('--emb_coef', default=3.0, type=float)\n",
        "parser.add_argument('--lambda_laplacian_smooth_loss', default=1.0, type=float)\n",
        "parser.add_argument('--use_11spk_only', default=False, action='store_true')\n",
        "parser.add_argument('-f')\n",
        "opt_parser = parser.parse_args()\n",
        "\n",
        "img = cv2.imread('examples/' + opt_parser.jpg)\n",
        "predictor = face_alignment.FaceAlignment(face_alignment.LandmarksType._3D, device='cpu', flip_input=True)\n",
        "shapes = predictor.get_landmarks(img)\n",
        "if (not shapes or len(shapes) != 1):\n",
        "    print('Cannot detect face landmarks. Exit.')\n",
        "    exit(-1)\n",
        "shape_3d = shapes[0]\n",
        "if(opt_parser.close_input_face_mouth):\n",
        "    util.close_input_face_mouth(shape_3d)\n",
        "shape_3d[48:, 0] = (shape_3d[48:, 0] - np.mean(shape_3d[48:, 0])) * LIP_WIDTH_ADJUST + np.mean(shape_3d[48:, 0]) # wider lips\n",
        "shape_3d[49:54, 1] -= UPPER_LIP_ADJUST           # thinner upper lip\n",
        "shape_3d[55:60, 1] += LOWER_LIP_ADJUST           # thinner lower lip\n",
        "shape_3d[[37,38,43,44], 1] -=2.    # larger eyes\n",
        "shape_3d[[40,41,46,47], 1] +=2.    # larger eyes\n",
        "shape_3d, scale, shift = util.norm_input_face(shape_3d)\n",
        "\n",
        "print(\"Loaded Image...\", file=sys.stderr)\n",
        "\n",
        "au_data = []\n",
        "au_emb = []\n",
        "ains = glob.glob1('examples', '*.wav')\n",
        "ains = [item for item in ains if item is not 'tmp.wav']\n",
        "ains.sort()\n",
        "for ain in ains:\n",
        "    os.system('ffmpeg -y -loglevel error -i examples/{} -ar 16000 examples/tmp.wav'.format(ain))\n",
        "    shutil.copyfile('examples/tmp.wav', 'examples/{}'.format(ain))\n",
        "\n",
        "    # au embedding\n",
        "    from thirdparty.resemblyer_util.speaker_emb import get_spk_emb\n",
        "    me, ae = get_spk_emb('examples/{}'.format(ain))\n",
        "    au_emb.append(me.reshape(-1))\n",
        "\n",
        "    print('Processing audio file', ain)\n",
        "    c = AutoVC_mel_Convertor('examples')\n",
        "\n",
        "    au_data_i = c.convert_single_wav_to_autovc_input(audio_filename=os.path.join('examples', ain),\n",
        "           autovc_model_path=opt_parser.load_AUTOVC_name)\n",
        "    au_data += au_data_i\n",
        "if(os.path.isfile('examples/tmp.wav')):\n",
        "    os.remove('examples/tmp.wav')\n",
        "\n",
        "print(\"Loaded audio...\", file=sys.stderr)\n",
        "\n",
        "# landmark fake placeholder\n",
        "fl_data = []\n",
        "rot_tran, rot_quat, anchor_t_shape = [], [], []\n",
        "for au, info in au_data:\n",
        "    au_length = au.shape[0]\n",
        "    fl = np.zeros(shape=(au_length, 68 * 3))\n",
        "    fl_data.append((fl, info))\n",
        "    rot_tran.append(np.zeros(shape=(au_length, 3, 4)))\n",
        "    rot_quat.append(np.zeros(shape=(au_length, 4)))\n",
        "    anchor_t_shape.append(np.zeros(shape=(au_length, 68 * 3)))\n",
        "\n",
        "if(os.path.exists(os.path.join('examples', 'dump', 'random_val_fl.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_fl.pickle'))\n",
        "if(os.path.exists(os.path.join('examples', 'dump', 'random_val_fl_interp.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_fl_interp.pickle'))\n",
        "if(os.path.exists(os.path.join('examples', 'dump', 'random_val_au.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_au.pickle'))\n",
        "if (os.path.exists(os.path.join('examples', 'dump', 'random_val_gaze.pickle'))):\n",
        "    os.remove(os.path.join('examples', 'dump', 'random_val_gaze.pickle'))\n",
        "\n",
        "with open(os.path.join('examples', 'dump', 'random_val_fl.pickle'), 'wb') as fp:\n",
        "    pickle.dump(fl_data, fp)\n",
        "with open(os.path.join('examples', 'dump', 'random_val_au.pickle'), 'wb') as fp:\n",
        "    pickle.dump(au_data, fp)\n",
        "with open(os.path.join('examples', 'dump', 'random_val_gaze.pickle'), 'wb') as fp:\n",
        "    gaze = {'rot_trans':rot_tran, 'rot_quat':rot_quat, 'anchor_t_shape':anchor_t_shape}\n",
        "    pickle.dump(gaze, fp)\n",
        "\n",
        "model = Audio2landmark_model(opt_parser, jpg_shape=shape_3d)\n",
        "if(len(opt_parser.reuse_train_emb_list) == 0):\n",
        "    model.test(au_emb=au_emb)\n",
        "else:\n",
        "    model.test(au_emb=None)\n",
        "\n",
        "print(\"Audio->Landmark...\", file=sys.stderr)\n",
        "\n",
        "fls = glob.glob1('examples', 'pred_fls_*.txt')\n",
        "fls.sort()\n",
        "\n",
        "for i in range(0,len(fls)):\n",
        "    fl = np.loadtxt(os.path.join('examples', fls[i])).reshape((-1, 68,3))\n",
        "    fl[:, :, 0:2] = -fl[:, :, 0:2]\n",
        "    fl[:, :, 0:2] = fl[:, :, 0:2] / scale - shift\n",
        "\n",
        "    if (ADD_NAIVE_EYE):\n",
        "        fl = util.add_naive_eye(fl)\n",
        "\n",
        "    # additional smooth\n",
        "    fl = fl.reshape((-1, 204))\n",
        "    fl[:, :48 * 3] = savgol_filter(fl[:, :48 * 3], 15, 3, axis=0)\n",
        "    fl[:, 48*3:] = savgol_filter(fl[:, 48*3:], 5, 3, axis=0)\n",
        "    fl = fl.reshape((-1, 68, 3))\n",
        "\n",
        "    ''' STEP 6: Imag2image translation '''\n",
        "    model = Image_translation_block(opt_parser, single_test=True)\n",
        "    with torch.no_grad():\n",
        "        model.single_test(jpg=img, fls=fl, filename=fls[i], prefix=opt_parser.jpg.split('.')[0])\n",
        "        print('finish image2image gen')\n",
        "    os.remove(os.path.join('examples', fls[i]))\n",
        "\n",
        "    print(\"{} / {}: Landmark->Face...\".format(i+1, len(fls)), file=sys.stderr)\n",
        "print(\"Done!\", file=sys.stderr)"
      ],
      "id": "vAfQWE6jO1IF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx83jnNsX7ts"
      },
      "source": [
        "!pip freeze | cat > requirements.txt "
      ],
      "id": "bx83jnNsX7ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07FWqzIJU5sy"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets==1.4.1\n",
        "!pip install transformers==4.4.2\n",
        "!pip install torchaudio\n",
        "!pip install librosa\n",
        "!pip install jiwer"
      ],
      "id": "07FWqzIJU5sy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORtmDPR1bUmj"
      },
      "source": [
        "%%timeit\n",
        "import torch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "def map_to_array(batch):\n",
        "    speech, _ = sf.read(batch[\"file\"])\n",
        "    batch[\"speech\"] = speech\n",
        "    return batch\n",
        "\n",
        "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "ds = ds.map(map_to_array)\n",
        "\n",
        "input_values = processor(ds[\"speech\"][0], return_tensors=\"pt\").input_values  # Batch size 1\n",
        "logits = model(input_values).logits\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "transcription = processor.decode(predicted_ids[0])\n",
        "\n",
        "# compute loss\n",
        "target_transcription = \"A MAN SAID TO THE UNIVERSE SIR I EXIST\"\n",
        "\n",
        "# wrap processor as target processor to encode labels\n",
        "with processor.as_target_processor():\n",
        "    labels = processor(transcription, return_tensors=\"pt\").input_ids\n",
        "\n",
        "loss = model(input_values, labels=labels).loss\n",
        "transcription"
      ],
      "id": "ORtmDPR1bUmj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJvMnDnJY5-X"
      },
      "source": [
        "def map_to_array(batch):\n",
        "     speech, _ = sf.read(batch[\"file\"])\n",
        "     batch[\"speech\"] = speech\n",
        "     return batch\n",
        "\n",
        "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "ds = ds.map(map_to_array)\n",
        "\n",
        "input_values = processor(ds[\"speech\"][0], return_tensors=\"pt\").input_values  # Batch size 1\n",
        "hidden_states = model(input_values).last_hidden_state"
      ],
      "id": "iJvMnDnJY5-X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKC7LMMn7k7m"
      },
      "source": [
        "#Chinese\n"
      ],
      "id": "zKC7LMMn7k7m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vqws5RzBC9CH"
      },
      "source": [
        ""
      ],
      "id": "Vqws5RzBC9CH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO8kYeIGDcuT"
      },
      "source": [
        ""
      ],
      "id": "TO8kYeIGDcuT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI6OI0m6B-X7"
      },
      "source": [
        ""
      ],
      "id": "YI6OI0m6B-X7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz94In6nDwI3"
      },
      "source": [
        ""
      ],
      "id": "pz94In6nDwI3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "022dKBB5D5TY"
      },
      "source": [
        ""
      ],
      "id": "022dKBB5D5TY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cICFsl8cEEOX"
      },
      "source": [
        ""
      ],
      "id": "cICFsl8cEEOX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJRqQhIAF1SJ"
      },
      "source": [
        ""
      ],
      "id": "rJRqQhIAF1SJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm1m74gCFBHZ"
      },
      "source": [
        ""
      ],
      "id": "fm1m74gCFBHZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cyP1VFy9ZMa"
      },
      "source": [
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };            \n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {            \n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data); \n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "      \n",
        "</script>\n",
        "\"\"\""
      ],
      "id": "5cyP1VFy9ZMa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtRvHu4J9iRx"
      },
      "source": [
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import wave\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import numpy as np\n",
        "import ffmpeg\n",
        "\n",
        "def write_wav(f, sr, x, normalized=False):\n",
        "    f = wave.open(f, \"wb\")\n",
        "    f.setnchannels(1)\n",
        "    f.setsampwidth(2)\n",
        "    f.setframerate(sr)\n",
        "    \n",
        "    wave_data = x.astype(np.short)\n",
        "    f.writeframes(wave_data.tobytes())\n",
        "    f.close()\n",
        "\n",
        "def get_audio():\n",
        "  global hnum\n",
        "\n",
        "  # call microphone\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "\n",
        "  process = (ffmpeg\n",
        "      .input('pipe:0')\n",
        "      .output('pipe:1', format='wav')\n",
        "      .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "\n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "  # save\n",
        "  human_sound_file = \"demo.wav\"\n",
        "  write_wav(human_sound_file, sr, audio)\n",
        "\n",
        "  return human_sound_file"
      ],
      "id": "XtRvHu4J9iRx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8VfU0Ta9l2S"
      },
      "source": [
        "import torchaudio\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import (\n",
        "    Wav2Vec2ForCTC,\n",
        "    Wav2Vec2Processor,\n",
        ")\n",
        "import torch\n",
        "import re\n",
        "import sys\n",
        "\n",
        "model_name = \"voidful/wav2vec2-large-xlsr-53-hk\"\n",
        "device = \"cuda\"\n",
        "processor_name = \"voidful/wav2vec2-large-xlsr-53-hk\"\n",
        "\n",
        "chars_to_ignore_regex = r\"[¥•＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､　、〃〈〉《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏﹑﹔·'℃°•·．﹑︰〈〉─《﹖﹣﹂﹁﹔！？｡。＂＃＄％＆＇（）＊＋，﹐－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.．!\\\"#$%&()*+,\\-.\\:;<=>?@\\[\\]\\\\\\/^_`{|}~]\"\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n",
        "processor = Wav2Vec2Processor.from_pretrained(processor_name)"
      ],
      "id": "s8VfU0Ta9l2S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVjB-P1o9n_h"
      },
      "source": [
        "resampler = torchaudio.transforms.Resample(orig_freq=48_000, new_freq=16_000)\n",
        "\n",
        "def load_file_to_data(file):\n",
        "    batch = {}\n",
        "    speech, _ = torchaudio.load(file)\n",
        "    batch[\"speech\"] = resampler.forward(speech.squeeze(0)).numpy()\n",
        "    batch[\"sampling_rate\"] = resampler.new_freq\n",
        "    return batch\n",
        "\n",
        "\n",
        "def predict(data):\n",
        "    features = processor(data[\"speech\"], sampling_rate=data[\"sampling_rate\"], padding=True, return_tensors=\"pt\")\n",
        "    input_values = features.input_values.to(device)\n",
        "    attention_mask = features.attention_mask.to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values, attention_mask=attention_mask).logits\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    return processor.batch_decode(pred_ids)"
      ],
      "id": "QVjB-P1o9n_h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_6MxboF9p8J"
      },
      "source": [
        "get_audio()"
      ],
      "id": "g_6MxboF9p8J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WiSh_md9sL_"
      },
      "source": [
        "predict(load_file_to_data('./demo.wav'))"
      ],
      "id": "5WiSh_md9sL_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cldcsU04-EBz"
      },
      "source": [
        "!pip install mecab-python3\n",
        "!pip install unidic-lite\n",
        "!python -m unidic download\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "from datasets import load_dataset\n",
        "import MeCab\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import re\n",
        "\n",
        "# config\n",
        "wakati = MeCab.Tagger(\"-Owakati\")\n",
        "chars_to_ignore_regex = '[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\、\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\。\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\．\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\「\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\」\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\…\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\？\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\・]'\n",
        "\n",
        "# load data, processor and model\n",
        "test_dataset = load_dataset(\"common_voice\", \"ja\", split=\"test[:2%]\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"vumichien/wav2vec2-large-xlsr-japanese\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"vumichien/wav2vec2-large-xlsr-japanese\")\n",
        "resampler = lambda sr, y: librosa.resample(y.numpy().squeeze(), sr, 16_000)\n",
        "\n",
        "# Preprocessing the datasets.\n",
        "def speech_file_to_array_fn(batch):\n",
        "    batch[\"sentence\"] = wakati.parse(batch[\"sentence\"]).strip()\n",
        "    batch[\"sentence\"] = re.sub(chars_to_ignore_regex,'', batch[\"sentence\"]).strip()\n",
        "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
        "    batch[\"speech\"] = resampler(sampling_rate, speech_array).squeeze()\n",
        "    return batch\n",
        "test_dataset = test_dataset.map(speech_file_to_array_fn)\n",
        "inputs = processor(test_dataset[\"speech\"][:2], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
        "with torch.no_grad():\n",
        "    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "print(\"Prediction:\", processor.batch_decode(predicted_ids))\n",
        "print(\"Reference:\", test_dataset[\"sentence\"][:2])"
      ],
      "id": "cldcsU04-EBz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioPI9jML-MsD"
      },
      "source": [
        "!pip install mecab-python3\n",
        "!pip install unidic-lite\n",
        "!python -m unidic download\n",
        "\n",
        "import torch\n",
        "import librosa\n",
        "import torchaudio\n",
        "from datasets import load_dataset, load_metric\n",
        "import MeCab\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import re\n",
        "\n",
        "#config\n",
        "wakati = MeCab.Tagger(\"-Owakati\")\n",
        "chars_to_ignore_regex = '[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\、\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\。\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\．\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\「\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\」\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\…\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\？\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\・]'\n",
        "\n",
        "# load data, processor and model\n",
        "test_dataset = load_dataset(\"common_voice\", \"ja\", split=\"test\")\n",
        "wer = load_metric(\"wer\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"vumichien/wav2vec2-large-xlsr-japanese\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"vumichien/wav2vec2-large-xlsr-japanese\")\n",
        "model.to(\"cuda\")\n",
        "resampler = lambda sr, y: librosa.resample(y.numpy().squeeze(), sr, 16_000)\n",
        "\n",
        "# Preprocessing the datasets.\n",
        "def speech_file_to_array_fn(batch):\n",
        "    batch[\"sentence\"] = wakati.parse(batch[\"sentence\"]).strip()\n",
        "    batch[\"sentence\"] = re.sub(chars_to_ignore_regex,'', batch[\"sentence\"]).strip()\n",
        "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
        "    batch[\"speech\"] = resampler(sampling_rate, speech_array).squeeze()\n",
        "    return batch\n",
        "test_dataset = test_dataset.map(speech_file_to_array_fn)\n",
        "\n",
        "# evaluate function\n",
        "def evaluate(batch):\n",
        "    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n",
        "    return batch\n",
        "result = test_dataset.map(evaluate, batched=True, batch_size=8)\n",
        "print(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))"
      ],
      "id": "ioPI9jML-MsD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC-iTg1F-uuj"
      },
      "source": [
        ""
      ],
      "id": "DC-iTg1F-uuj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULPfLNJP_fXm"
      },
      "source": [
        "import torch\n",
        "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "\n",
        "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "processor = Speech2Textprocessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "\n",
        "def map_to_array(batch):\n",
        "    speech, _ = sf.read(batch[\"file\"])\n",
        "    batch[\"speech\"] = speech\n",
        "    return batch\n",
        "\n",
        "ds = load_dataset(\n",
        "    \"patrickvonplaten/librispeech_asr_dummy\",\n",
        "    \"clean\",\n",
        "    split=\"validation\"\n",
        ")\n",
        "ds = ds.map(map_to_array)\n",
        "\n",
        "input_features = processor(\n",
        "    ds[\"speech\"][0],\n",
        "    sampling_rate=16_000,\n",
        "    return_tensors=\"pt\"\n",
        ").input_features  # Batch size 1\n",
        "generated_ids = model.generate(input_ids=input_features)\n",
        "\n",
        "transcription = processor.batch_decode(generated_ids)"
      ],
      "id": "ULPfLNJP_fXm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiEF9KFbK_gZ"
      },
      "source": [
        "import torch\n",
        "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "\n",
        "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "processor = Speech2Textprocessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "\n",
        "def map_to_array(batch):\n",
        "    speech, _ = sf.read(batch[\"file\"])\n",
        "    batch[\"speech\"] = speech\n",
        "    return batch\n",
        "\n",
        "ds = load_dataset(\n",
        "    \"patrickvonplaten/librispeech_asr_dummy\",\n",
        "    \"clean\",\n",
        "    split=\"validation\"\n",
        ")\n",
        "ds = ds.map(map_to_array)\n",
        "\n",
        "input_features = processor(\n",
        "    ds[\"speech\"][0],\n",
        "    sampling_rate=16_000,\n",
        "    return_tensors=\"pt\"\n",
        ").input_features  # Batch size 1\n",
        "generated_ids = model.generate(input_ids=input_features)\n",
        "\n",
        "transcription = processor.batch_decode(generated_ids)"
      ],
      "id": "IiEF9KFbK_gZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIf4Du5pLxQR"
      },
      "source": [
        ""
      ],
      "id": "BIf4Du5pLxQR",
      "execution_count": null,
      "outputs": []
    }
  ]
}